{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac87c9",
   "metadata": {},
   "source": [
    "## Initialization of model, constants, paths etc.\n",
    "Initialization of necessary constants and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef4c9c9b-6892-4590-8b29-f6859b6497cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "(7534739456, 47842000896)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "model_translate_path = config['TST']['model_translate_path']\n",
    "model_summarize_path = config['TST']['model_summarize_path']\n",
    "libcuda_path = config['Unsloth']['libcuda_path']\n",
    "library_path = config['Unsloth']['library_path']\n",
    "dataset_path = config['TST']['dataset_path']\n",
    "output_path = config['TST']['output_path']\n",
    "\n",
    "import os\n",
    "os.environ[\"TRITON_LIBCUDA_PATH\"]=libcuda_path\n",
    "os.environ[\"LIBRARY_PATH\"]=library_path\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "import rouge_raw\n",
    "eval = rouge_raw.RougeRaw()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "print(torch.cuda.mem_get_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ec2a57-02f1-4835-a076-0973231c0bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, max_seq_length = 32768): #\n",
    "    max_seq_length = max_seq_length # Choose any! We auto support RoPE Scaling internally! (dont choose any or ALMA generates nonsense, for alma choose = 2048\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0dbaf21-e880-4b17-84e8-7c0837d448a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A40. Max memory = 44.556 GB.\n",
      "37.217 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6967cd2-5129-4f78-9559-52906a099bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alma_prompt(texts, src_lang=\"Czech\", out_lang=\"English\"):\n",
    "    formatted_texts = texts.copy()\n",
    "    for i, text in enumerate(texts):\n",
    "        formatted_texts[i] = f\"Translate this from {src_lang} to {out_lang}:\\n{src_lang}: {text}\\n{out_lang}:\"\n",
    "    return formatted_texts\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\"-\\n\",\"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path, 'r') as j:\n",
    "         contents = json.loads(j.read())\n",
    "    return contents\n",
    "\n",
    "def chunk_sentences(text, n, lang):\n",
    "    # Initialize an empty list to store the joined sentences\n",
    "    sentence_split = nltk.sent_tokenize(text=text, language=lang)\n",
    "    joined_sentences = []\n",
    "    \n",
    "    # Iterate over the sentences list with a step of n\n",
    "    for i in range(0, len(sentence_split), n):\n",
    "        # Join the current and the next n-1 sentences and append to the list\n",
    "        joined_sentences.append(' '.join(sentence_split[i:i+n]))\n",
    "    \n",
    "    return joined_sentences\n",
    "\n",
    "def translate_text(prompt, model, tokenizer, max_new_tokens=2048, temperature=1, top_p=1, repetition_penalty = 1.3):\n",
    "    inputs = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens = max_new_tokens)\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True, temperature=temperature, top_p=top_p, repetition_penalty = repetition_penalty)\n",
    "    #print(decoded_outputs)\n",
    "    return decoded_outputs[0].split(prompt)[1]\n",
    "\n",
    "def translate_article(text, model, tokenizer, prompt_func, src_lang, out_lang, max_new_tokens=2048, chunk_size=10):\n",
    "    text = clean_text(text)\n",
    "    chunked_sentences = chunk_sentences(text, chunk_size, \"czech\")\n",
    "    formatted_chunked_sentences = prompt_func(chunked_sentences, src_lang, out_lang)\n",
    "    #print(formatted_chunked_sentences)\n",
    "    translated_article = []\n",
    "    for i, chunk_sentence in enumerate(formatted_chunked_sentences):\n",
    "        #print(f\"{i}/{len(formatted_chunked_sentences)}\")\n",
    "        translated_text = translate_text(chunk_sentence, model, tokenizer, max_new_tokens)\n",
    "        translated_article.append(translated_text)\n",
    "    return ''.join(translated_article)\n",
    "\n",
    "def translate_dataset(dataset_path, \n",
    "                      output_path, \n",
    "                      model, \n",
    "                      tokenizer,\n",
    "                      prompt_func,\n",
    "                      max_new_tokens=2048, \n",
    "                      save_steps=-1, \n",
    "                      src_lang=\"Czech\", \n",
    "                      out_lang=\"English\", \n",
    "                      page_key=\"text\", \n",
    "                      page_key_translated=\"text_translated\", issue_key=None, issue_key_translated=None):\n",
    "    pages_processed = 0\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    journals_processed = 0\n",
    "    for key_journal, journal in dataset.items():\n",
    "        print(f\"Processing journal {key_journal}, {journals_processed}/{len(dataset.items())}:\") \n",
    "        journals_processed += 1\n",
    "        for key_issue, issues in journal.items():\n",
    "            print(f\"Processing issue {key_issue}:\") \n",
    "            for key_page, pages in issues.items():\n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    text_to_translate = pages[i][page_key]\n",
    "                    translated_page = translate_article(text_to_translate, model, tokenizer, prompt_func, src_lang, out_lang, max_new_tokens)\n",
    "                    print(translated_page)\n",
    "                    pages[i][page_key_translated] = translated_page\n",
    "                    pages_processed += 1\n",
    "                    if save_steps > 0 and pages_processed % save_steps == 0:\n",
    "                        filename = f\"translated_{os.path.splitext(os.path.basename(dataset_path))[0]}\"\n",
    "                        with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "                            print(\"Saving checkpoint\")\n",
    "                            myfile.write(json.dumps(dataset))\n",
    "            if issue_key is not None and issue_key_translated is not None:\n",
    "                text_to_translate = issues[issue_key]\n",
    "                translated_page = translate_article(text_to_translate, model, tokenizer, prompt_func, src_lang, out_lang, max_new_tokens)\n",
    "                issues[issue_key_translated] = translated_page\n",
    "                \n",
    "    with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "        myfile.write(json.dumps(dataset))\n",
    "        print(f\"Finished translating. Saved to {output_path}/{filename}.json\")\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb63c144-d3e0-4b72-a959-8f8ee5bf1458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_enumeration(text):\n",
    "    pattern = r'^\\s*\\d+\\.\\s*'\n",
    "    cleaned_lines = [re.sub(pattern, '', line) for line in text.split('\\n')]\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def summarize_dataset(dataset_path, \n",
    "                      output_path, \n",
    "                      model, \n",
    "                      tokenizer,\n",
    "                      prompt_func,\n",
    "                      max_new_tokens=512, \n",
    "                      save_steps=-1, \n",
    "                      src_lang=\"Czech\", \n",
    "                      out_lang=\"English\", \n",
    "                      page_key=\"text_translated\", \n",
    "                      page_key_summary=\"summary_translated\"):\n",
    "    pages_processed = 0\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    journals_processed = 0\n",
    "    for key_journal, journal in dataset.items():\n",
    "        print(f\"Summarization: Processing journal {key_journal}, {journals_processed}/{len(dataset.items())}:\") \n",
    "        journals_processed += 1\n",
    "        for key_issue, issues in journal.items():\n",
    "            print(f\"Processing issue {key_issue}:\") \n",
    "            issue_summary_total = []\n",
    "            for key_page, pages in issues.items():\n",
    "                \n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    text_to_summarize = clean_text(pages[i][page_key])\n",
    "                    if len(text_to_summarize.split()) > 10:\n",
    "                        summarized_page = remove_enumeration(summarize_text(prompt_func(text_to_summarize), model, tokenizer, max_new_tokens))\n",
    "                    else:\n",
    "                        summarized_page = \" \"\n",
    "                    pages[i][page_key_summary] = summarized_page\n",
    "                    print(summarized_page)\n",
    "                    issue_summary_total.append(summarized_page)\n",
    "                    pages_processed += 1\n",
    "                    if save_steps > 0 and pages_processed % save_steps == 0:\n",
    "                        filename = f\"summarized_{os.path.splitext(os.path.basename(dataset_path))[0]}\"\n",
    "                        with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "                            print(\"Saving checkpoint\")\n",
    "                            myfile.write(json.dumps(dataset))\n",
    "            text_to_summarize = clean_text('\\n'.join(issue_summary_total))\n",
    "            summarized_issue = remove_enumeration(summarize_text(prompt_func(text_to_summarize), model, tokenizer, max_new_tokens))\n",
    "            print(summarized_issue)\n",
    "            issues[\"summary_total_translated\"] = summarized_issue\n",
    "    with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "        myfile.write(json.dumps(dataset))\n",
    "        print(f\"Finished summarizing. Saved to {output_path}/{filename}.json\")\n",
    "        \n",
    "def summarize_text(prompt, model, tokenizer, max_new_tokens=512, temperature=0.3, top_p=1):\n",
    "    #print(tokenizer.apply_chat_template(prompt, tokenize=False, return_tensors = \"pt\").to(\"cuda\"))\n",
    "    inputs = tokenizer.apply_chat_template(prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(inputs, max_new_tokens = max_new_tokens)\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True, temperature=temperature, top_p=top_p)\n",
    "    #print(decoded_outputs[0].split(prompt))\n",
    "    split = decoded_outputs[0].split(\"[/INST]\")\n",
    "    return split[len(split)-1]\n",
    "\n",
    "def mistral_prompt(text):\n",
    "    text = f\"Summarize the following text in five sentences: {text}\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize my texts using only 5 sentences\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Sure. I will write summaries in the style of a news reporter and use only 5 sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{text}\"},\n",
    "    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ae6634f-5744-4425-b003-918fdf3f9e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_path, \n",
    "                      page_gold_key = \"summary\", \n",
    "                      page_system_key =\"summary_reference\", issue_gold_key=\"summary_total\", issue_reference_key = \"summary_total_reference\"):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    summary_total_gold = []\n",
    "    summary_total_reference = []\n",
    "    page_gold = []\n",
    "    page_system = []\n",
    "    for key_journal, journal in dataset.items():\n",
    "        for key_issue, issues in journal.items():\n",
    "            for key_page, pages in issues.items():\n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    #print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    gold = pages[i][page_gold_key]\n",
    "                    system = pages[i][page_system_key]\n",
    "                    page_gold.append(gold)\n",
    "                    page_system.append(system)\n",
    "            summary_total_gold.append(issues[issue_gold_key])\n",
    "            summary_total_reference.append(issues[\"summary_total_reference\"])\n",
    "    summary_eval = eval.corpus(gold=page_gold, system=page_system)\n",
    "    summary_total_eval = eval.corpus(gold=summary_total_gold, system=summary_total_reference)\n",
    "    return summary_eval, summary_total_eval\n",
    "\n",
    "def prefix_filename(path, prefix):\n",
    "    dir_name, file_name = os.path.split(path)\n",
    "    prefixed_file_name = prefix + file_name\n",
    "    return os.path.join(dir_name, prefixed_file_name)\n",
    "\n",
    "def print_rougeraw(score):\n",
    "    print(\"ROUGE-1 F: \", score[\"1\"].f*100)\n",
    "    print(\"ROUGE-1 P: \", score[\"1\"].p*100)\n",
    "    print(\"ROUGE-1 R: \", score[\"1\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-2 F: \", score[\"2\"].f*100)\n",
    "    print(\"ROUGE-2 P: \", score[\"2\"].p*100)\n",
    "    print(\"ROUGE-2 R: \", score[\"2\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-L F: \", score[\"L\"].f*100)\n",
    "    print(\"ROUGE-L P: \", score[\"L\"].p*100)\n",
    "    print(\"ROUGE-L R: \", score[\"L\"].r*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3eed8",
   "metadata": {},
   "source": [
    "Load translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faba5e74-6b32-4a3c-b865-3b6c64c76f35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.556 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4dd60a9ca84ca4a14e67e8e233ce2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c688ace18bbb49cf99199bb74d375d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at haoranxu/ALMA-13B-Pretrain were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_translate_path, max_seq_length=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e2a3e",
   "metadata": {},
   "source": [
    "## TST\n",
    "Translate POC dataset to English. Creates a copy of the dataset with prefix \"translated_\" with translated text to the set output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e252786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define filenames\n",
    "translated_dataset_path = prefix_filename(dataset_path, \"translated_\")\n",
    "summarized_translated_dataset_path = prefix_filename(translated_dataset_path, \"summarized_\")\n",
    "translated_summarized_translated_dataset_path = prefix_filename(summarized_translated_dataset_path, \"translated_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f5c85-1918-4644-999a-7b4f08c0f285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translate_dataset(dataset_path, \n",
    "                  output_path, \n",
    "                  model, \n",
    "                  tokenizer, \n",
    "                  alma_prompt, \n",
    "                  save_steps=5, \n",
    "                  src_lang=\"Czech\", \n",
    "                  out_lang=\"English\", \n",
    "                  page_key=\"text\", page_key_translated=\"text_translated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c2b81b",
   "metadata": {},
   "source": [
    "Load English summarization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c36fe-fede-43a2-8a60-46e113094744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_summarize_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b56d246",
   "metadata": {},
   "source": [
    "Summarize English text. Creates a copy of the dataset with prefix \"summarized_\" with summarized text to the set output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045bd04-60b0-41d2-9078-2308d781566a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_dataset(translated_dataset_path, output_path, model, tokenizer, mistral_prompt, save_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9ab13",
   "metadata": {},
   "source": [
    "Load translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae821a8-8492-4865-98fd-637594488f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_translate_path, max_seq_length=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609421db",
   "metadata": {},
   "source": [
    "Translate English summaries to Czech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ce8c0-0df4-43b9-8114-9ae845fa4505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "translate_dataset(summarized_translated_dataset_path, \n",
    "                  output_path, \n",
    "                  model, \n",
    "                  tokenizer, \n",
    "                  alma_prompt, \n",
    "                  save_steps=5, \n",
    "                  src_lang=\"English\", \n",
    "                  out_lang=\"Czech\", \n",
    "                  page_key=\"summary_translated\", page_key_translated=\"summary_reference\", issue_key=\"summary_total_translated\", issue_key_translated=\"summary_total_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ce9e0",
   "metadata": {},
   "source": [
    "## Evaluation of TST\n",
    "Evaluate the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7553c2e4-7c2e-4a37-875b-68573f82fc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = evaluate_dataset(translated_summarized_translated_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1218b-2bb2-45e8-aa73-fac49a65530e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"page\")\n",
    "print_rougeraw(scores[0])\n",
    "print(\"total\")\n",
    "print_rougeraw(scores[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
