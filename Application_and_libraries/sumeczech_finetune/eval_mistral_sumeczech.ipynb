{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae768fb",
   "metadata": {},
   "source": [
    "## Initialization of model, constants, paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4c9c9b-6892-4590-8b29-f6859b6497cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "(27853062144, 47842000896)\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "dataset_storage_path = config['Mistral-SC']['dataset_storage_path']\n",
    "model_storage_path = config['Mistral-SC']['model_storage_path']\n",
    "output_path = config['Mistral-SC']['output_dir']\n",
    "libcuda_path = config['Unsloth']['libcuda_path']\n",
    "library_path = config['Unsloth']['library_path']\n",
    "\n",
    "import torch\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "print(torch.cuda.mem_get_info())\n",
    "\n",
    "import os\n",
    "os.environ[\"TRITON_LIBCUDA_PATH\"]=libcuda_path\n",
    "os.environ[\"LIBRARY_PATH\"]=library_path\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 4096*2\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_storage_path, \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c1e89-2173-4342-b5ae-9b8a7f2cd707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    #instructions = examples[\"instruction\"]\n",
    "    instruction = \"Summarize the following text:\"\n",
    "    inputs       = examples[\"text\"]\n",
    "    outputs      = examples[\"abstract\"]\n",
    "    instructions = [instruction for _ in range(len(inputs))]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"instruction\": instructions, \"input\": inputs, \"output\": outputs,  \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "data_files = {\"test\":f\"{dataset_storage_path}/sumeczech-1.0-test.jsonl\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "test_dataset = dataset[\"test\"]\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched = True).select_columns([\"input\", \"output\", \"text\", \"instruction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe4038",
   "metadata": {},
   "source": [
    "## Generate summaries for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73ceaeb-7785-429f-aa65-ad35fab24509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "for sample in test_dataset:\n",
    "    formatted_sample = alpaca_prompt.format(\n",
    "        \"Summarize the following text:\", # instruction\n",
    "        sample[\"input\"], # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "    articles.append(formatted_sample)\n",
    "import json\n",
    "generated_outputs = []\n",
    "with open(f\"{output_path}/generated_outputs_temp.json\", \"r\", encoding=\"utf-8\") as readfile:\n",
    "    x = json.loads(readfile.read())\n",
    "    generated_outputs = x if isinstance(x, list) else []\n",
    "already_processed_len = (len(generated_outputs))\n",
    "for i, article in enumerate(articles[already_processed_len:]):\n",
    "    inputs = tokenizer([article], truncation=True, max_length = max_seq_length, return_tensors = \"pt\").to(\"cuda\")\n",
    "    if inputs[\"input_ids\"].size(1) >= max_seq_length:\n",
    "        decoded = tokenizer.batch_decode(inputs[\"input_ids\"])\n",
    "        new_article = f\"{decoded[0]}\\n### Response:\\n\" \n",
    "        inputs = tokenizer([new_article], return_tensors = \"pt\").to(\"cuda\")\n",
    "        print(inputs[\"input_ids\"].size(1))\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    #print(result)\n",
    "    for res in result:\n",
    "        splitted_res = res.split(\"### Response:\")\n",
    "        generated_outputs.append(splitted_res[1].split(\"</s>\")[0].replace(\"\\n\",\"\"))\n",
    "        with open(f\"{output_path}/generated_outputs_temp.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(json.dumps(generated_outputs))\n",
    "    print(f\"Progress: {already_processed_len+i+1}/{len(articles)}\")\n",
    "#generated_outputs\n",
    "generated_outputs_json = json.dumps(generated_outputs)\n",
    "with open(f\"{output_path}/generated_outputs.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(generated_outputs_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fd532",
   "metadata": {},
   "source": [
    "## Evaluate (get rouge score of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b30eca0-697a-48ff-b328-43e3fd54ec8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import rouge_raw\n",
    "eval = rouge_raw.RougeRaw()\n",
    "abstract = dataset[\"test\"][\"abstract\"]\n",
    "roguerawscore = eval.corpus(gold=abstract, system=generated_outputs)\n",
    "\n",
    "with open(f\"{output_path}/eval_sumeczech.txt\", \"w\") as file:\n",
    "    file.write(\"ROUGE-1 F: \" + str(roguerawscore[\"1\"].f * 100) + \"\\n\")\n",
    "    file.write(\"ROUGE-1 P: \" + str(roguerawscore[\"1\"].p * 100) + \"\\n\")\n",
    "    file.write(\"ROUGE-1 R: \" + str(roguerawscore[\"1\"].r * 100) + \"\\n\")\n",
    "\n",
    "    file.write(\"ROUGE-2 F: \" + str(roguerawscore[\"2\"].f * 100) + \"\\n\")\n",
    "    file.write(\"ROUGE-2 P: \" + str(roguerawscore[\"2\"].p * 100) + \"\\n\")\n",
    "    file.write(\"ROUGE-2 R: \" + str(roguerawscore[\"2\"].r * 100) + \"\\n\")\n",
    "\n",
    "    file.write(\"ROUGE-L F: \" + str(roguerawscore[\"L\"].f * 100) + \"\\n\")\n",
    "    file.write(\"ROUGE-L P: \" + str(roguerawscore[\"L\"].p * 100) + \"\\n\")\n",
    "    file.write(\"ROUGE-L R: \" + str(roguerawscore[\"L\"].r * 100) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
