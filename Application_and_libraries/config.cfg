[Unsloth]
#set if root/sudo is not available
#path to directory where libcuda.so resides
libcuda_path = /usr/local/cuda/compat

#path to cuda library
library_path = /usr/local/cuda/lib64


[SumeCzech]
#path to the directory with sumeczech dataset with train, dev and test files
dataset_dir = PLACE_YOUR_SUMECZECH_DATASET_LOCATION_HERE

#test, train and dev filenames
test = sumeczech-1.0-test.jsonl
train = sumeczech-1.0-train.jsonl
dev = sumeczech-1.0-dev.jsonl

[mT5]
#Summarization
#Repository of trained mT5 on sumeczech
summarizer = tranv/mt5-base-finetuned-sumeczech

#Training
#Training vars
base_model = google/mt5-base

#where the model should be saved
output_dir = ../Results/models

batch_size = 8
num_train_epochs = 8
learning_rate = 0.001
weight_decay = 0.01

#api_token is required if use_huggingface is set to True
api_token = INSERT_YOUR_TOKEN_HERE
use_huggingface = False
resume_training = False

#Evaluation
output_path = .
dataset_path = ../../Input_data/dataset_poc.json

[M7B_SC]
#path to the directory that contains the dataset with train, dev and test files
dataset_storage_path = PLACE_YOUR_DATASET_LOCATION_HERE

#where the fine-tuned model should be saved/retrieved
model_storage_path = ./mistral_sc_lora_model

#the directory, where the output should be saved. Can be "." for the current directory
output_dir = ../../Results

[M7B_POC_EVAL]
#poc_p dataset
dataset_storage_path = ../../Input_data/dataset_poc_p.json

#directory where all the checkpoints are stored
checkpoint_eval_output_dir = ../../Results/m7b_poc_checkpoints

#the dataset that should be used for evaluation must be in this directory
#where the output evaluation should be saved 
eval_output_dir = ../../Input_data

#the model to be evaluated
base_model_name = checkpoint-step-120-epoch-3

[M7B_POC_FINETUNE]
#training dataset (poc_p)
dataset_storage_path = ../../Input_data/dataset_poc_p.json

#the model to be fine-tuned
base_model_name = ../sumeczech_finetune/mistral_sc_lora_model

#where the checkpoints should be saved
output_dir = ../../Results/m7b_poc_checkpoints

#where the fine-tuned model should be saved
model_storage_path = ../../Results/m7b_poc_checkpoints/m7b_poc_final_checkpoint


[TST]
#the machine translation model
model_translate_path = haoranxu/ALMA-13B-R

#the summarization model (changing this will require corresponding changes in the code for formatting the input)
model_summarize_path = unsloth/mistral-7b-instruct-v0.2-bnb-4bit

#the path to the poc dataset (or a dataset in the same format)
dataset_path = ../../Input_data/dataset_poc.json

#where the output should be saved
output_path = ../../Results

[POC_Dataset]
#the prefix of .txt files that contain page text of an Posel od Cerchova issue
TEXT_PREFIX = posel-od-cerchova-

#suffix of the folder that is to be processed (e.g. posel-od-cerchova-1882-ukazka)
FOLDER_SUFFIX = -ukazka

#the prefix of the summary file
SUMMARY_PREFIX = summary-

#the prompt that will be fed to LLM
PROMPT = Vytvoř shrnutí následujícího textu ve stylu novináře. Počet vět <= 5:\n

#name of the file that contains the total summary
SUMMARY_TOTAL = summary-total.txt

#name of the file that contains the prompt for the total summary
SUMMARY_TOTAL_PROMPT = summary-total-prompt.txt

#where the resulting dataset should be saved
DATASET_POC = ../Results/dataset_poc.json

#where the resulting dataset (but only pages, no issues/total summaries), should be saved
DATASET_POC_P = ../Results/dataset_poc_p.json

#directory that contains Posel od Cerchova journals, sorted by year
TEXT_LOC = ../Input_data/posel-od-cerchova