{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674f68f4",
   "metadata": {},
   "source": [
    "## Initialization of model, constants, paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c9c9b-6892-4590-8b29-f6859b6497cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "model_summarize_path = config['mT5']['summarizer']\n",
    "libcuda_path = config['Unsloth']['libcuda_path']\n",
    "library_path = config['Unsloth']['library_path']\n",
    "dataset_path = config['mT5']['dataset_path']\n",
    "output_path = config['mT5']['output_path']\n",
    "\n",
    "import os\n",
    "os.environ[\"TRITON_LIBCUDA_PATH\"]=libcuda_path\n",
    "os.environ[\"LIBRARY_PATH\"]=library_path\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import rouge_raw\n",
    "eval = rouge_raw.RougeRaw()\n",
    "\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "print(torch.cuda.mem_get_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ec2a57-02f1-4835-a076-0973231c0bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, max_seq_length = 512): #\n",
    "    summarizer_t5 = pipeline(task='summarization', model=model_path, max_length = max_seq_length, device_map=\"auto\")\n",
    "    return summarizer_t5\n",
    "def clean_text(text):\n",
    "    return text.replace(\"-\\n\",\"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path, 'r') as j:\n",
    "         contents = json.loads(j.read())\n",
    "    return contents\n",
    "\n",
    "def remove_enumeration(text):\n",
    "    pattern = r'^\\s*\\d+\\.\\s*'\n",
    "    cleaned_lines = [re.sub(pattern, '', line) for line in text.split('\\n')]\n",
    "    return '\\n'.join(cleaned_lines)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb63c144-d3e0-4b72-a959-8f8ee5bf1458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset_path, \n",
    "                      output_path, \n",
    "                      pipeline,\n",
    "                      prompt_func,\n",
    "                      max_new_tokens=1024, \n",
    "                      save_steps=-1, \n",
    "                      page_text_key=\"text\", \n",
    "                      page_summary_key=\"summary\", issue_summary_total_key=\"summary_total\", overwrite = False):\n",
    "    pages_processed = 0\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    journals_processed = 0\n",
    "    for key_journal, journal in dataset.items():\n",
    "        print(f\"Summarization: Processing journal {key_journal}, {journals_processed}/{len(dataset.items())}:\") \n",
    "        journals_processed += 1\n",
    "        for key_issue, issues in journal.items():\n",
    "            print(f\"Processing issue {key_issue}:\") \n",
    "            issue_summary_total = []\n",
    "            for key_page, pages in issues.items():\n",
    "                \n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    if page_summary_key in pages[i] and overwrite is False:\n",
    "                        continue\n",
    "                    text_to_summarize = clean_text(pages[i][page_text_key])\n",
    "                    if len(text_to_summarize.split()) > 10:\n",
    "                        summarized_page = (summarize_text(prompt_func(text_to_summarize), pipeline, max_new_tokens))\n",
    "                    else:\n",
    "                        summarized_page = \" \"\n",
    "                    pages[i][page_summary_key] = summarized_page\n",
    "                    print(summarized_page)\n",
    "                    issue_summary_total.append(summarized_page)\n",
    "                    pages_processed += 1\n",
    "                    if save_steps > 0 and pages_processed % save_steps == 0:\n",
    "                        filename = f\"summarized_{os.path.splitext(os.path.basename(dataset_path))[0]}\"\n",
    "                        with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "                            print(\"Saving checkpoint\")\n",
    "                            myfile.write(json.dumps(dataset))\n",
    "            if issue_summary_total_key in issues and overwrite is False:\n",
    "                continue\n",
    "            text_to_summarize = clean_text('\\n'.join(issue_summary_total))\n",
    "            summarized_issue = (summarize_text(prompt_func(text_to_summarize), pipeline, max_new_tokens))\n",
    "            print(summarized_issue)\n",
    "            issues[issue_summary_total_key] = summarized_issue\n",
    "    with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "        myfile.write(json.dumps(dataset))\n",
    "        print(f\"Finished summarizing. Saved to {output_path}/{filename}.json\")\n",
    "        \n",
    "def summarize_text(prompt, pipeline, max_new_tokens=512, temperature=1, top_p = 1, do_sample = True, num_beams = 4, top_k = 50, repetition_penalty = 1.2):\n",
    "    res = pipeline(prompt, temperature=temperature, top_p = top_p, do_sample=do_sample, num_beams=num_beams, top_k = top_k, repetition_penalty = repetition_penalty, no_repeat_ngram_size = 4 )\n",
    "    return res[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def t5_prompt(text):\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae6634f-5744-4425-b003-918fdf3f9e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_path, \n",
    "                      page_gold_key = \"summary\", \n",
    "                      page_system_key =\"summary_reference\", issue_gold_key=\"summary_total\", issue_reference_key = \"summary_total_reference\"):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    summary_total_gold = []\n",
    "    summary_total_reference = []\n",
    "    page_gold = []\n",
    "    page_system = []\n",
    "    for key_journal, journal in dataset.items():\n",
    "        for key_issue, issues in journal.items():\n",
    "            for key_page, pages in issues.items():\n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    #print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    gold = pages[i][page_gold_key]\n",
    "                    system = pages[i][page_system_key]\n",
    "                    page_gold.append(gold)\n",
    "                    page_system.append(system)\n",
    "            summary_total_gold.append(issues[issue_gold_key])\n",
    "            summary_total_reference.append(issues[issue_reference_key])\n",
    "    summary_eval = eval.corpus(gold=page_gold, system=page_system)\n",
    "    summary_total_eval = eval.corpus(gold=summary_total_gold, system=summary_total_reference)\n",
    "    return summary_eval, summary_total_eval\n",
    "\n",
    "def print_rougeraw(score):\n",
    "    print(\"ROUGE-1 F: \", score[\"1\"].f*100)\n",
    "    print(\"ROUGE-1 P: \", score[\"1\"].p*100)\n",
    "    print(\"ROUGE-1 R: \", score[\"1\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-2 F: \", score[\"2\"].f*100)\n",
    "    print(\"ROUGE-2 P: \", score[\"2\"].p*100)\n",
    "    print(\"ROUGE-2 R: \", score[\"2\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-L F: \", score[\"L\"].f*100)\n",
    "    print(\"ROUGE-L P: \", score[\"L\"].p*100)\n",
    "    print(\"ROUGE-L R: \", score[\"L\"].r*100)\n",
    "    \n",
    "def write_rougeraw_to_file(score, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"ROUGE-1 F: \" + str(score[\"1\"].f*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-1 P: \" + str(score[\"1\"].p*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-1 R: \" + str(score[\"1\"].r*100) + \"\\n\")\n",
    "\n",
    "        file.write(\"ROUGE-2 F: \" + str(score[\"2\"].f*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-2 P: \" + str(score[\"2\"].p*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-2 R: \" + str(score[\"2\"].r*100) + \"\\n\")\n",
    "\n",
    "        file.write(\"ROUGE-L F: \" + str(score[\"L\"].f*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-L P: \" + str(score[\"L\"].p*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-L R: \" + str(score[\"L\"].r*100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0eafed",
   "metadata": {},
   "source": [
    "## Generate summaries for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c36fe-fede-43a2-8a60-46e113094744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = load_model(model_summarize_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045bd04-60b0-41d2-9078-2308d781566a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_dataset(dataset_path, output_path, pipeline, t5_prompt, save_steps=5, page_summary_key=\"summary_mt5\", issue_summary_total_key=\"summary_total_mt5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f84841",
   "metadata": {},
   "source": [
    "## Evaluate (get rouge score of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553c2e4-7c2e-4a37-875b-68573f82fc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basename = f\"summarized_{os.path.splitext(os.path.basename(dataset_path))[0]}\"\n",
    "filename = f\"{output_path}/{basename}.json\"\n",
    "scores = evaluate_dataset(filename, page_system_key =\"summary_mt5\", issue_reference_key=\"summary_total_mt5\")\n",
    "print(\"summary\")\n",
    "write_rougeraw_to_file(scores[0], f\"{output_path}/score_pages_poc_dataset.txt\")\n",
    "print_rougeraw(scores[0])\n",
    "print(\"summary total\")\n",
    "write_rougeraw_to_file(scores[1], f\"{output_path}/score_issues_poc_dataset.txt\")\n",
    "print_rougeraw(scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
