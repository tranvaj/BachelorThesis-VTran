{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f9cd8e",
   "metadata": {},
   "source": [
    "Initialization of necessary constants and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e4081f-db3c-41df-9d54-371d8a86294d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "/usr/local/cuda/lib64\n",
      "\n",
      "/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "libcuda_path = \"/usr/local/cuda/compat\" #path to directory where libcuda.so resides\n",
    "library_path = \"/usr/local/cuda/lib64\" #path to cuda library\n",
    "dataset_storage_path = \"/home/meta/nuva/unsloth/dataset_poc_p.json\" #path to where the POC dataset is stored\n",
    "output_dir = \"/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4\" #path to the output directory\n",
    "\n",
    "#workaround for the need of root permissions when linking libcuda.so\n",
    "os.environ[\"TRITON_LIBCUDA_PATH\"]=libcuda_path\n",
    "os.environ[\"LIBRARY_PATH\"]=library_path\n",
    "!echo $LD_LIBRARY_PATH\n",
    "!echo $LIBRARY_PATH\n",
    "import rouge_raw\n",
    "eval = rouge_raw.RougeRaw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2801a2-986e-441e-b3aa-4a57010a438e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe MIG 2g.20gb. Max memory: 19.5 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 01:50:20.169051: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 01:50:20.169175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 01:50:20.171665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 01:50:20.182578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 01:50:22.089449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 4096*2 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "base_model_name = \"/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-120\"\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14534f72",
   "metadata": {},
   "source": [
    "Uses dataset containing only page summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa67c31-46e9-4b92-8a71-23acd5d2815d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'text', 'instruction'],\n",
       "        num_rows: 324\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'text', 'instruction'],\n",
       "        num_rows: 108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    #instructions = examples[\"instruction\"]\n",
    "    instruction = \"Summarize the following text:\"\n",
    "    inputs       = examples[\"text\"]\n",
    "    outputs      = examples[\"summary\"]\n",
    "    instructions = [instruction for _ in range(len(inputs))]\n",
    "    texts = []\n",
    "    for i, input in enumerate(inputs):\n",
    "        inputs[i] = inputs[i].replace(\"-\\n\",\"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"instruction\": instructions, \"input\": inputs, \"output\": outputs,  \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "data_files = {\"train\":f\"{dataset_storage_path}\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"].split()) > 5)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True).select_columns([\"input\", \"output\", \"text\", \"instruction\"]) #single train dataset\n",
    "\n",
    "dataset = train_dataset.train_test_split(test_size=0.25, shuffle=False) #train test dataset 90/10\n",
    "\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': dataset['test'],})\n",
    "    #'dev': dataset['test']})\n",
    "\n",
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a4a8e47-f4fd-475e-aabd-9e732741f95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#evaluate dataset using rouge_raw\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 4096*2 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def model_inference(model_path, text_to_summarize):\n",
    "    print(f\"Loading {model_path}...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    formatted_sample = alpaca_prompt.format(\n",
    "            \"Summarize the following text:\", # instruction\n",
    "            text_to_summarize, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "    inputs = tokenizer([formatted_sample], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "    return result[0].split(\"### Response:\")[1].split(\"</s>\")[0].replace(\"\\n\",\"\")\n",
    "\n",
    "def push_model_to_hub(model_path, repo_path, hf_token):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    model.push_to_hub(repo_path, token = hf_token) # Online saving\n",
    "\n",
    "def eval_dataset(model_path, test_dataset):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    \n",
    "    articles = []\n",
    "    for sample in train_test_valid_dataset[\"test\"][\"input\"]:\n",
    "        formatted_sample = alpaca_prompt.format(\n",
    "            \"Summarize the following text:\", # instruction\n",
    "            sample, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "        articles.append(formatted_sample)\n",
    "\n",
    "    generated_outputs = []\n",
    "    for i, article in enumerate(articles):\n",
    "        print(f\"{i}/{len(articles)}\")\n",
    "        #if(i == 0 or i == 1):\n",
    "            #continue #test\n",
    "        #print(article)\n",
    "        inputs = tokenizer([article], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "        result = tokenizer.batch_decode(outputs)\n",
    "        for res in result:\n",
    "            summary_res = res.split(\"### Response:\")[1].split(\"</s>\")[0].replace(\"\\n\",\"\")\n",
    "            #print(summary_res)\n",
    "            generated_outputs.append(summary_res)\n",
    "    \n",
    "    with open(f\"{model_path}/generated_outputs.json\", \"w\") as file:\n",
    "        json_gen_outputs = json.dumps(generated_outputs)\n",
    "        file.write(json_gen_outputs)\n",
    "\n",
    "    roguerawscore = eval.corpus(gold=test_dataset, system=generated_outputs)\n",
    "    with open(f\"{model_path}/rouge_scores.txt\", \"w\") as file:\n",
    "        # Write the ROUGE-1 scores\n",
    "        file.write(\"ROUGE-1 F: \" + str(roguerawscore[\"1\"].f * 100) + \"\\n\")\n",
    "        file.write(\"ROUGE-1 P: \" + str(roguerawscore[\"1\"].p * 100) + \"\\n\")\n",
    "        file.write(\"ROUGE-1 R: \" + str(roguerawscore[\"1\"].r * 100) + \"\\n\")\n",
    "\n",
    "        # Write the ROUGE-2 scores\n",
    "        file.write(\"ROUGE-2 F: \" + str(roguerawscore[\"2\"].f * 100) + \"\\n\")\n",
    "        file.write(\"ROUGE-2 P: \" + str(roguerawscore[\"2\"].p * 100) + \"\\n\")\n",
    "        file.write(\"ROUGE-2 R: \" + str(roguerawscore[\"2\"].r * 100) + \"\\n\")\n",
    "\n",
    "        # Write the ROUGE-L scores\n",
    "        file.write(\"ROUGE-L F: \" + str(roguerawscore[\"L\"].f * 100) + \"\\n\")\n",
    "        file.write(\"ROUGE-L P: \" + str(roguerawscore[\"L\"].p * 100) + \"\\n\")\n",
    "        file.write(\"ROUGE-L R: \" + str(roguerawscore[\"L\"].r * 100) + \"\\n\")\n",
    "        \n",
    "def evaluate_model_paths(paths, test_dataset):\n",
    "    for i, path in enumerate(paths):\n",
    "        print(f\"evaluating: {path}\")\n",
    "        eval_dataset(path, test_dataset)\n",
    "        print(f\"evaluated {i} out of {len(paths)}\")\n",
    "\n",
    "        \n",
    "def create_total_scores(checkpoint_paths):\n",
    "    with open(f\"{output_dir}/total_scores.txt\", \"w\") as myfile:\n",
    "        text_to_write = \"\"\n",
    "        for checkpoint in checkpoint_paths:\n",
    "            basename = os.path.basename(checkpoint)\n",
    "            print(basename)\n",
    "            rougescore_path = f\"{checkpoint}/rouge_scores.txt\"\n",
    "            with open(rougescore_path) as file:\n",
    "                text = file.read()\n",
    "            text_to_write += f\"{basename}\\n{text}\\n\\n\"\n",
    "        myfile.write(text_to_write)\n",
    "    \n",
    "def print_rougeraw(score):\n",
    "    print(\"ROUGE-1 F: \", score[\"1\"].f*100)\n",
    "    print(\"ROUGE-1 P: \", score[\"1\"].p*100)\n",
    "    print(\"ROUGE-1 R: \", score[\"1\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-2 F: \", score[\"2\"].f*100)\n",
    "    print(\"ROUGE-2 P: \", score[\"2\"].p*100)\n",
    "    print(\"ROUGE-2 R: \", score[\"2\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-L F: \", score[\"L\"].f*100)\n",
    "    print(\"ROUGE-L P: \", score[\"L\"].p*100)\n",
    "    print(\"ROUGE-L R: \", score[\"L\"].r*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22119b76",
   "metadata": {},
   "source": [
    "Evaluation of each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77b4475-c9a5-47b3-8616-2704ac901f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-40',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-80',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-120',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-160',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-200',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-240',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-280',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-320',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-360',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-400',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-440',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-480',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-520',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-560',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-600',\n",
       " '/home/meta/nuva/unsloth/mistral_finetune_sc_poc_75_25_2e-4/checkpoint-640']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dirs = [f\"{output_dir}/{dir}\" for dir in os.listdir(output_dir) if dir.startswith(\"checkpoint\")]\n",
    "sorted_paths = sorted(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))\n",
    "sorted_paths\n",
    "evaluate_model_paths(sorted_paths, train_test_valid_dataset[\"test\"][\"output\"]) #rouge scores saved in each checkpoint directory\n",
    "create_total_scores(sorted_paths) #total scores saved in output directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
