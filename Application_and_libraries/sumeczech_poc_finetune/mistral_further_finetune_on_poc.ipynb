{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8f4e6",
   "metadata": {},
   "source": [
    "## Initialization of model, constants, paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b847f1-f87a-481c-bbdf-30f813cf5cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "(47560982528, 47842000896)\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "dataset_storage_path = config['M7B-POC-FINETUNE']['dataset_storage_path'] #requires poc_p dataset\n",
    "output_path_name = config['M7B-POC-FINETUNE']['output_path_name']\n",
    "base_model_name = config['M7B-POC-FINETUNE']['base_model_name']\n",
    "output_dir = config['M7B-POC-FINETUNE']['output_dir']\n",
    "model_storage_path = config['M7B-POC-FINETUNE']['model_storage_path']\n",
    "libcuda_path = config['Unsloth']['libcuda_path'] #path to directory where libcuda.so resides\n",
    "library_path = config['Unsloth']['library_path'] #path to cuda library\n",
    "\n",
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "print(torch.cuda.mem_get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e4081f-db3c-41df-9d54-371d8a86294d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "/usr/local/cuda/lib64\n",
      "\n",
      "/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_LIBCUDA_PATH\"]=libcuda_path\n",
    "os.environ[\"LIBRARY_PATH\"]=library_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2801a2-986e-441e-b3aa-4a57010a438e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.556 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "2024-03-20 10:11:57.902458: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-20 10:11:57.902546: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-20 10:11:57.904087: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-20 10:11:57.909188: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 10:11:58.722048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 4096*2 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa67c31-46e9-4b92-8a71-23acd5d2815d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'text', 'instruction'],\n",
       "        num_rows: 324\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'text', 'instruction'],\n",
       "        num_rows: 108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    #instructions = examples[\"instruction\"]\n",
    "    instruction = \"Summarize the following text:\"\n",
    "    inputs       = examples[\"text\"]\n",
    "    outputs      = examples[\"summary\"]\n",
    "    instructions = [instruction for _ in range(len(inputs))]\n",
    "    texts = []\n",
    "    for i, input in enumerate(inputs):\n",
    "        inputs[i] = inputs[i].replace(\"-\\n\",\"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"instruction\": instructions, \"input\": inputs, \"output\": outputs,  \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "data_files = {\"train\":f\"{dataset_storage_path}\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"].split()) > 5)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True).select_columns([\"input\", \"output\", \"text\", \"instruction\"]) #single train dataset\n",
    "\n",
    "dataset = train_dataset.train_test_split(test_size=0.25, shuffle=False) #train test dataset 90/10\n",
    "\n",
    "\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': dataset['test'],})\n",
    "    #'dev': dataset['test']})\n",
    "\n",
    "train_test_valid_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4a8e47-f4fd-475e-aabd-9e732741f95e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'text', 'instruction'],\n",
       "    num_rows: 108\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(train_test_valid_dataset[\"train\"][0])\n",
    "#train_test_valid_dataset[\"test\"][0][\"input\"]\n",
    "train_test_valid_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32700ec4",
   "metadata": {},
   "source": [
    "### Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd0957e-d4f7-4c6b-b54f-29db2803dcb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_device_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 16\n",
    "total_steps = (len(dataset['train']) // (per_device_batch_size*gradient_accumulation_steps)) * num_train_epochs\n",
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726a1a0a-b594-4066-82b8-c193290b9787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_test_valid_dataset[\"train\"],\n",
    "    #eval_dataset = train_test_valid_dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        #default\n",
    "        per_device_train_batch_size = per_device_batch_size,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        warmup_steps = total_steps//10,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = total_steps//16,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 727,\n",
    "        output_dir = output_dir,\n",
    "\n",
    "        #save strategy\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = total_steps//16,\n",
    "        save_total_limit=20\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b1251",
   "metadata": {},
   "source": [
    "## Training and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899129e1-f9b4-491b-9b2d-986fbbf8b5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "model.save_pretrained(model_storage_path)\n",
    "#model.push_to_hub(\"\", token = \"\") # Online saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
