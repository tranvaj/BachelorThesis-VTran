{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f75095",
   "metadata": {},
   "source": [
    "## Initialization of model, constants, paths etc.\n",
    "Initialization of necessary constants and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4c9c9b-6892-4590-8b29-f6859b6497cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "(20791492608, 20937965568)\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "\n",
    "model_summarize_path = config['M7B_POC_EVAL']['base_model_name']\n",
    "libcuda_path = config['Unsloth']['libcuda_path'] #path to directory where libcuda.so resides\n",
    "library_path = config['Unsloth']['library_path'] #path to cuda library\n",
    "output_path = config['M7B_POC_EVAL']['eval_output_dir'] #\"/home/jovyan/nuva/unsloth/mistral/ver3\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TRITON_LIBCUDA_PATH\"]=libcuda_path\n",
    "os.environ[\"LIBRARY_PATH\"]=library_path\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "import rouge_raw\n",
    "eval = rouge_raw.RougeRaw()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "print(torch.cuda.mem_get_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ec2a57-02f1-4835-a076-0973231c0bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, max_seq_length = 32768): #\n",
    "    max_seq_length = max_seq_length # Choose any! We auto support RoPE Scaling internally! (dont choose any or ALMA generates nonsense, for alma choose = 2048\n",
    "    dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dbaf21-e880-4b17-84e8-7c0837d448a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100 80GB PCIe MIG 2g.20gb. Max memory = 19.5 GB.\n",
      "0.0 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432f9ec2-78f1-4358-b45d-8143eb8d8fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.replace(\"-\\n\",\"\").replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path, 'r') as j:\n",
    "         contents = json.loads(j.read())\n",
    "    return contents\n",
    "\n",
    "def remove_enumeration(text):\n",
    "    pattern = r'^\\s*\\d+\\.\\s*'\n",
    "    cleaned_lines = [re.sub(pattern, '', line) for line in text.split('\\n')]\n",
    "    return '\\n'.join(cleaned_lines)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb63c144-d3e0-4b72-a959-8f8ee5bf1458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset_path, \n",
    "                      output_path, \n",
    "                      model, \n",
    "                      tokenizer,\n",
    "                      prompt_func,\n",
    "                      max_new_tokens=512, \n",
    "                      save_steps=-1, \n",
    "                      page_text_key=\"text\", \n",
    "                      page_summary_key=\"summary\", issue_summary_total_key=\"summary_total\", overwrite = True):\n",
    "    pages_processed = 0\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    journals_processed = 0\n",
    "    for key_journal, journal in dataset.items():\n",
    "        print(f\"Summarization: Processing journal {key_journal}, {journals_processed}/{len(dataset.items())}:\") \n",
    "        journals_processed += 1\n",
    "        for key_issue, issues in journal.items():\n",
    "            print(f\"Processing issue {key_issue}:\") \n",
    "            issue_summary_total = []\n",
    "            for key_page, pages in issues.items():\n",
    "                \n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    if page_summary_key in pages[i] and overwrite is False:\n",
    "                        continue\n",
    "                    text_to_summarize = clean_text(pages[i][page_text_key])\n",
    "                    if len(text_to_summarize.split()) > 10:\n",
    "                        #print(prompt_func(text_to_summarize))\n",
    "                        summarized_page = (summarize_text(prompt_func(text_to_summarize), model, tokenizer, max_new_tokens, temperature=1, top_p=1, do_sample=False, repetition_penalty = 1))\n",
    "                    else:\n",
    "                        summarized_page = \"\"\n",
    "                    pages[i][page_summary_key] = summarized_page\n",
    "                    print(summarized_page)\n",
    "                    issue_summary_total.append(summarized_page)\n",
    "                    pages_processed += 1\n",
    "                    if save_steps > 0 and pages_processed % save_steps == 0:\n",
    "                        filename = f\"summarized_{os.path.splitext(os.path.basename(dataset_path))[0]}\"\n",
    "                        with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "                            print(\"Saving checkpoint\")\n",
    "                            myfile.write(json.dumps(dataset))\n",
    "            if issue_summary_total_key in issues and overwrite is False:\n",
    "                continue\n",
    "            text_to_summarize = clean_text('\\n'.join(issue_summary_total))\n",
    "            summarized_issue = (summarize_text(prompt_func(text_to_summarize), model, tokenizer, max_new_tokens, temperature=0.6, top_p=0.8, do_sample=True, num_beams=1, top_k = 50, repetition_penalty = 1.1))\n",
    "            print(summarized_issue)\n",
    "            issues[issue_summary_total_key] = summarized_issue\n",
    "    with open(f\"{output_path}/{filename}.json\", \"w\") as myfile:\n",
    "        myfile.write(json.dumps(dataset))\n",
    "        print(f\"Finished summarizing. Saved to {output_path}/{filename}.json\")\n",
    "        \n",
    "def summarize_text(prompt, model, tokenizer, max_new_tokens=512, temperature=1, top_p=1, do_sample=True, num_beams=1, top_k = 50, repetition_penalty = 1):\n",
    "    max_seq_length = 8192\n",
    "    inputs = tokenizer([prompt], max_length=max_seq_length , truncation=True, return_tensors = \"pt\").to(\"cuda\")\n",
    "    #inputs = tokenizer([articles[i]], truncation=True, max_length = max_seq_length, return_tensors = \"pt\").to(\"cuda\")\n",
    "    #print(articles[i])\n",
    "    if inputs[\"input_ids\"].size(1) >= max_seq_length:\n",
    "        decoded = tokenizer.batch_decode(inputs[\"input_ids\"])\n",
    "        new_article = f\"{decoded[0]}\\n### Response:\\n\" \n",
    "        inputs = tokenizer([new_article], return_tensors = \"pt\").to(\"cuda\")\n",
    "        print(inputs[\"input_ids\"].size(1))\n",
    "    outputs = model.generate(**inputs, max_new_tokens = max_new_tokens, temperature = temperature, top_p = top_p, do_sample = do_sample, num_beams = num_beams, top_k = top_k, repetition_penalty = repetition_penalty)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    res_split = result[0].split(\"### Response:\")\n",
    "    return res_split[len(res_split)-1].replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "def mistral_prompt(text):\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "    text = alpaca_prompt.format(\"Summarize the following text:\", text, \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae6634f-5744-4425-b003-918fdf3f9e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_path, \n",
    "                      page_gold_key = \"summary\", \n",
    "                      page_system_key =\"summary_reference\", issue_gold_key=\"summary_total\", issue_reference_key = \"summary_total_reference\"):\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    summary_total_gold = []\n",
    "    summary_total_reference = []\n",
    "    page_gold = []\n",
    "    page_system = []\n",
    "    for key_journal, journal in dataset.items():\n",
    "        for key_issue, issues in journal.items():\n",
    "            for key_page, pages in issues.items():\n",
    "                if key_page.startswith(\"summary_total\"):\n",
    "                    continue\n",
    "                for i, page in enumerate(pages): \n",
    "                    #print(f\"Processing {key_journal}, {key_issue}, page {i} out of {len(pages)}:\")\n",
    "                    gold = pages[i][page_gold_key]\n",
    "                    system = pages[i][page_system_key]\n",
    "                    if not system or not system.strip():\n",
    "                        continue\n",
    "                    page_gold.append(gold)\n",
    "                    page_system.append(system)\n",
    "            summary_total_gold.append(issues[issue_gold_key])\n",
    "            summary_total_reference.append(issues[issue_reference_key])\n",
    "    summary_eval = eval.corpus(gold=page_gold, system=page_system)\n",
    "    summary_total_eval = eval.corpus(gold=summary_total_gold, system=summary_total_reference)  \n",
    "    return summary_eval, summary_total_eval\n",
    "\n",
    "def print_rougeraw(score):\n",
    "    print(\"ROUGE-1 F: \", score[\"1\"].f*100)\n",
    "    print(\"ROUGE-1 P: \", score[\"1\"].p*100)\n",
    "    print(\"ROUGE-1 R: \", score[\"1\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-2 F: \", score[\"2\"].f*100)\n",
    "    print(\"ROUGE-2 P: \", score[\"2\"].p*100)\n",
    "    print(\"ROUGE-2 R: \", score[\"2\"].r*100)\n",
    "\n",
    "    print(\"ROUGE-L F: \", score[\"L\"].f*100)\n",
    "    print(\"ROUGE-L P: \", score[\"L\"].p*100)\n",
    "    print(\"ROUGE-L R: \", score[\"L\"].r*100)\n",
    "    \n",
    "def write_rougeraw_to_file(score, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"ROUGE-1 F: \" + str(score[\"1\"].f*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-1 P: \" + str(score[\"1\"].p*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-1 R: \" + str(score[\"1\"].r*100) + \"\\n\")\n",
    "\n",
    "        file.write(\"ROUGE-2 F: \" + str(score[\"2\"].f*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-2 P: \" + str(score[\"2\"].p*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-2 R: \" + str(score[\"2\"].r*100) + \"\\n\")\n",
    "\n",
    "        file.write(\"ROUGE-L F: \" + str(score[\"L\"].f*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-L P: \" + str(score[\"L\"].p*100) + \"\\n\")\n",
    "        file.write(\"ROUGE-L R: \" + str(score[\"L\"].r*100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bd628",
   "metadata": {},
   "source": [
    "Load the model that will summarize the dataset. Reminder that Unsloth does not support all models available on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9c36fe-fede-43a2-8a60-46e113094744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe MIG 2g.20gb. Max memory: 19.5 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 08:41:56.710157: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-02 08:41:56.710280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-02 08:41:56.712881: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-02 08:41:56.724502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 08:41:58.910288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_summarize_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79933ddc",
   "metadata": {},
   "source": [
    "## Evaluate on entire POC (even the summaries the model has been trained on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045bd04-60b0-41d2-9078-2308d781566a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_dataset(f\"{output_path}/dataset.json\", output_path, model, tokenizer, mistral_prompt, save_steps=5, page_summary_key=\"summary_mistral_poc\", issue_summary_total_key=\"summary_total_mistral_poc\", overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553c2e4-7c2e-4a37-875b-68573f82fc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = evaluate_dataset(f\"{output_path}/summarized_dataset.json\", page_system_key =\"summary_mistral_poc\", issue_reference_key=\"summary_total_mistral_poc\")\n",
    "print(\"summary\")\n",
    "write_rougeraw_to_file(eval_results[0], f\"{output_path}/score_summary.txt\")\n",
    "print_rougeraw(eval_results[0])\n",
    "print(\"summary total\")\n",
    "write_rougeraw_to_file(eval_results[1], f\"{output_path}/score_summary_total.txt\")\n",
    "print_rougeraw(eval_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5114b",
   "metadata": {},
   "source": [
    "## Evaluate on POC test set (106 page summaries, 25 issue summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc234e1-eb52-4a68-84bb-4acb93dda006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_dataset(f\"{output_path}/dataset_poc_106.json\", output_path, model, tokenizer, mistral_prompt, save_steps=5, page_summary_key=\"summary_mistral_poc\", issue_summary_total_key=\"summary_total_mistral_poc\", overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0bc2dd5-ba7e-42ee-b2ee-490ab1f0ad95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary\n",
      "ROUGE-1 F:  19.599579570871565\n",
      "ROUGE-1 P:  23.513918267142934\n",
      "ROUGE-1 R:  17.38431438069711\n",
      "ROUGE-2 F:  3.9690679348681015\n",
      "ROUGE-2 P:  4.764066321875574\n",
      "ROUGE-2 R:  3.4995058932600855\n",
      "ROUGE-L F:  13.81292002879949\n",
      "ROUGE-L P:  16.566196019881907\n",
      "ROUGE-L R:  12.24982428608036\n",
      "summary total\n",
      "ROUGE-1 F:  16.596355391327243\n",
      "ROUGE-1 P:  17.466067926707165\n",
      "ROUGE-1 R:  16.567176445351826\n",
      "ROUGE-2 F:  2.4480608499240892\n",
      "ROUGE-2 P:  2.5795150330036583\n",
      "ROUGE-2 R:  2.4281210485098663\n",
      "ROUGE-L F:  11.751708190298409\n",
      "ROUGE-L P:  12.404226708262984\n",
      "ROUGE-L R:  11.78423937614917\n"
     ]
    }
   ],
   "source": [
    "eval_results = evaluate_dataset(f\"{output_path}/summarized_dataset_poc_106.json\", page_system_key =\"summary_mistral_poc\", issue_reference_key=\"summary_total_mistral_poc\")\n",
    "print(\"summary\")\n",
    "write_rougeraw_to_file(eval_results[0], f\"{output_path}/score_summary_106.txt\")\n",
    "print_rougeraw(eval_results[0])\n",
    "print(\"summary total\")\n",
    "write_rougeraw_to_file(eval_results[1], f\"{output_path}/score_summary_total_106.txt\")\n",
    "print_rougeraw(eval_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
